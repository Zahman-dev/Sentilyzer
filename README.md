# Sentilyzer - Finansal Duygu Analizi ve Geri Test Platformu

## Proje Vizyonu

Finansal metin verilerini, iÅŸlenebilir stratejik iÃ§gÃ¶rÃ¼lere dÃ¶nÃ¼ÅŸtÃ¼ren, Ã¶lÃ§eklenebilir, bakÄ±mÄ± kolay ve geniÅŸletilebilir bir platform yaratmak. Sistem, bir "veri fabrikasÄ±" gibi Ã§alÄ±ÅŸmalÄ±dÄ±r; bir uÃ§tan ham madde (metin) girer, montaj hatlarÄ±ndan (servisler) geÃ§er ve diÄŸer uÃ§tan deÄŸerli bir Ã¼rÃ¼n (analiz/strateji sonucu) Ã§Ä±kar.

## Temel Mimari

Sistem, **Olay GÃ¼dÃ¼mlÃ¼ (Event-Driven)** ve **Servis OdaklÄ± (Service-Oriented)** bir mimariye dayanÄ±r. Servisler birbirlerini doÄŸrudan Ã§aÄŸÄ±rmazlar; bunun yerine veritabanÄ±ndaki durum deÄŸiÅŸikliklerine (olaylara) tepki verirler. Bu, sistemin esnekliÄŸini ve dayanÄ±klÄ±lÄ±ÄŸÄ±nÄ± (resilience) en Ã¼st dÃ¼zeye Ã§Ä±karÄ±r.

## Temel TasarÄ±m KararlarÄ± ve GerekÃ§eleri

Bu proje, sadece belirli teknolojileri kullanmakla kalmaz, aynÄ± zamanda bu seÃ§imlerin arkasÄ±ndaki felsefeyi de benimser. Her bir teknoloji ve mimari desen, platformun uzun vadeli hedefleri olan **Ã¶lÃ§eklenebilirlik**, **bakÄ±m kolaylÄ±ÄŸÄ±** ve **dayanÄ±klÄ±lÄ±ÄŸÄ± (resilience)** desteklemek iÃ§in dikkatle seÃ§ilmiÅŸtir.

*   **Neden Servis OdaklÄ± Mimari (Service-Oriented Architecture)?**
    *   **Hata Ä°zolasyonu:** Bir servis (Ã¶rn: `sentiment_processor`) Ã§Ã¶kse bile, sistemin geri kalanÄ± (Ã¶rn: `data_ingestor`, `signals_api`) Ã§alÄ±ÅŸmaya devam eder.
    *   **BaÄŸÄ±msÄ±z Ã–lÃ§eklendirme:** EÄŸer duygu analizi iÅŸlemi yavaÅŸlarsa, sadece o iÅŸi yapan `celery_worker` sayÄ±sÄ±nÄ± artÄ±rÄ±rÄ±z. TÃ¼m platformu yatayda bÃ¼yÃ¼tmek zorunda kalmayÄ±z.
    *   **Teknoloji Ã–zgÃ¼rlÃ¼ÄŸÃ¼:** Gelecekte duygu analizi modelini Python yerine Rust ile yazmak istersek, bunu diÄŸer servisleri etkilemeden yapabiliriz.

*   **Neden VeritabanÄ± Ãœzerinden Olay GÃ¼dÃ¼mlÃ¼ Ä°letiÅŸim?**
    *   **DayanÄ±klÄ±lÄ±k:** Veri toplayÄ±cÄ± servis, veriyi veritabanÄ±na yazdÄ±ÄŸÄ± an gÃ¶revini tamamlamÄ±ÅŸ sayÄ±lÄ±r. Analiz servisi o an Ã§alÄ±ÅŸmÄ±yor olsa bile veri kaybolmaz; servis ayaÄŸa kalktÄ±ÄŸÄ±nda kaldÄ±ÄŸÄ± yerden devam eder.
    *   **Basitlik ve AyrÄ±klÄ±k (Decoupling):** Servislerin birbirlerinin aÄŸ adresini veya API'Ä±nÄ± bilmesi gerekmez. Tek iletiÅŸim noktasÄ± veritabanÄ±dÄ±r. Bu, sistemi Ã§ok daha basit ve yÃ¶netilebilir kÄ±lar.

*   **Neden Celery & Redis ile Asenkron GÃ¶rev YÃ¶netimi?**
    *   **API'Ä±n TÄ±kanmasÄ±nÄ± Ã–nleme:** Bir kullanÄ±cÄ±nÄ±n API isteÄŸiyle 10 dakika sÃ¼recek bir rapor oluÅŸturmak, kullanÄ±cÄ±yÄ± bekletir ve sistemi yorar. Celery ile bu iÅŸi anÄ±nda arka plana atÄ±p kullanÄ±cÄ±ya "iÅŸleminiz alÄ±ndÄ±, tamamlanÄ±nca haber vereceÄŸiz" yanÄ±tÄ± dÃ¶nebiliriz.
    *   **Ã–lÃ§eklenebilir Ä°ÅŸ YÃ¼kÃ¼:** GÃ¼nde 100 haber iÅŸlerken 1 `worker` yeterliyken, 1 milyon haber iÅŸlerken `worker` sayÄ±sÄ±nÄ± 50'ye Ã§Ä±kararak aynÄ± hÄ±zda Ã§alÄ±ÅŸmaya devam edebiliriz. Bu esneklik, `APScheduler` gibi servis-iÃ§i bir kÃ¼tÃ¼phane ile mÃ¼mkÃ¼n deÄŸildir.
    *   **GÃ¼venilir Zamanlama:** Periyodik veri toplama gÃ¶revleri, uygulama kodundan baÄŸÄ±msÄ±z, merkezi bir `Celery Beat` servisi tarafÄ±ndan yÃ¶netilir. Bu, zamanlamanÄ±n daha gÃ¼venilir ve yÃ¶netilebilir olmasÄ±nÄ± saÄŸlar.

*   **Neden Alembic ile VeritabanÄ± Versiyonlama?**
    *   **"Kod Olarak Åema" (Schema as Code):** VeritabanÄ± ÅŸemasÄ±nÄ± (tablolar, sÃ¼tunlar) kod gibi Git ile versiyonlamamÄ±zÄ± saÄŸlar.
    *   **TutarlÄ±lÄ±k ve GÃ¼venlik:** Her geliÅŸtiricinin bilgisayarÄ±nda ve production ortamÄ±nda aynÄ± veritabanÄ± ÅŸemasÄ±nÄ±n olmasÄ±nÄ± garanti eder. Manuel, hataya aÃ§Ä±k `ALTER TABLE...` komutlarÄ±nÄ± Ã§alÄ±ÅŸtÄ±rma riskini ortadan kaldÄ±rÄ±r.

*   **Neden Her Åey iÃ§in Docker?**
    *   **"Benim Makinemde Ã‡alÄ±ÅŸÄ±yordu" Sorununu Ã‡Ã¶zmek:** Bir geliÅŸtiricinin makinesinde Ã§alÄ±ÅŸan kodun, baÅŸka bir geliÅŸtiricinin makinesinde veya sunucuda da birebir aynÄ± ÅŸekilde Ã§alÄ±ÅŸacaÄŸÄ±nÄ± garanti eder.
    *   **Ä°zolasyon ve Kolay Kurulum:** Projenin tÃ¼m baÄŸÄ±mlÄ±lÄ±klarÄ± (PostgreSQL, Redis, Python kÃ¼tÃ¼phaneleri) konteynerler iÃ§inde izole edilir. Yeni bir geliÅŸtirici, sadece `docker-compose up` komutuyla tÃ¼m platformu 5 dakika iÃ§inde Ã§alÄ±ÅŸÄ±r hale getirebilir.

```
+---------------------------+
|      DIÅ DÃœNYA (SOURCES)  |
| - Reuters, Bloomberg (RSS)|
| - Twitter API             |
| - SEC Filings (EDGAR)     |
+-------------|-------------+
              |
              â–¼ (1. Veri AlÄ±mÄ±)
+---------------------------+
|    SERVICE 1:             |
|    DATA INGESTOR          |  <<-- (Periyodik olarak tetiklenir: CRON/Scheduler)
|---------------------------|
| Sorumluluk: Veriyi ham     |
| ve deÄŸiÅŸmemiÅŸ olarak     |
| standart bir formatta     |
| sisteme almak.            |
|                           |
| Ã‡Ä±ktÄ±: "raw_articles"     |
| tablosuna yeni kayÄ±t.     |
+-------------|-------------+
              |
              â–¼ (2. OLAY: Yeni Ham Veri Eklendi)
+---------------------------+
|    VERÄ°TABANI (PostgreSQL)|
|      - TEK GERÃ‡EK KAYNAÄI - |
+---------------------------+
| - raw_articles            |
| - sentiment_scores        |
| - backtest_results        |
| - (Gelecekte) price_data  |
+-------------|-------------+
              â–²
              | (3. OLAY: Yeni Ham Veri Bekleniyor)
              |
+-------------|-------------+
|    SERVICE 2:             |
|    SENTIMENT PROCESSOR    | <<-- (SÃ¼rekli veritabanÄ±nÄ± dinler: Polling)
|---------------------------|
| Sorumluluk: Ham metni      |
| alÄ±p, zenginleÅŸtirmek     |
| (duygu analizi).          |
|                           |
| Ã‡Ä±ktÄ±: "sentiment_scores" |
| tablosuna yeni kayÄ±t.     |
+-------------|-------------+
              â–²
              | (4. OLAY: Analiz Sonucu Bekleniyor)
              |
+-------------|-------------+
|    SERVICE 3:             |
|    SIGNALS API            | <<-- (KullanÄ±cÄ± tarafÄ±ndan tetiklenir: HTTP Request)
| (MVP'de "Duygu Veri SaÄŸlayÄ±cÄ±") |
|---------------------------|
| Sorumluluk: Analiz edilmiÅŸ|
| duygu skorlarÄ±nÄ± ve ilgili|
| verileri, isteÄŸe baÄŸlÄ±   |
| olarak sunmak.            |
|                           |
| Ã‡Ä±ktÄ±: JSON response.     |
+-------------|-------------+
              |
              â–¼ (5. API Ã‡aÄŸrÄ±sÄ±)
+---------------------------+
|    SERVICE 4:             |
|    DASHBOARD              |
|---------------------------|
| Sorumluluk: Ä°nsan-bilgisayar|
| etkileÅŸimini saÄŸlamak.    |
|                           |
| Ã‡Ä±ktÄ±: GÃ¶rsel arayÃ¼z.      |
+-------------|-------------+
              |
              â–¼
+---------------------------+
|        KULLANICI          |
+---------------------------+
```

## ğŸš€ Modern Development Environment Setup

This project uses a modern Python setup with `pyproject.toml` for dependency management and `ruff` for code quality.

### Prerequisites
- Python 3.10+
- [uv](https://github.com/astral-sh/uv): An extremely fast Python package installer and resolver.
  ```bash
  # On macOS and Linux
  curl -LsSf https://astral.sh/uv/install.sh | sh
  ```
- Docker and Docker Compose (for running the full system)
- Pre-commit for git hooks:
  ```bash
  pip install pre-commit
  ```

### 1. Local Setup (with `uv`)

This is the recommended way for local development and running tests.

```bash
# 1. Create and activate a virtual environment
uv venv

# On macOS/Linux
source .venv/bin/activate
# On Windows
.venv\Scripts\activate

# 2. Install all project dependencies (including all services and dev tools)
uv pip install -e ".[dev]"

# 3. Set up pre-commit hooks
pre-commit install
```

### 2. Running Services

#### With Docker (Recommended for Production & Staging)
The easiest way to run the entire platform is with Docker Compose. It handles the database, Redis, and all microservices.

```bash
# Build and start all services
docker-compose up --build
```

**Access Points:**
- **Dashboard**: `http://localhost:8501`
- **Signals API Docs**: `http://localhost:8888/docs`

#### Individually (for focused development)
If you want to run a single service locally (e.g., `signals_api`):

```bash
# Make sure you have installed dependencies as in step 1
# Run the FastAPI server with uvicorn
uvicorn services.signals_api.app.main:app --host 0.0.0.0 --port 8888 --reload
```
You will need a running PostgreSQL and Redis instance for this. The simplest way is to run them with Docker:
`docker-compose up -d postgres redis`

## ğŸ› ï¸ Developer Workflow

### Code Quality
We use `ruff` for linting and formatting.

```bash
# Format all files
ruff format .

# Check for linting errors and apply fixes
ruff check . --fix
```
The pre-commit hooks will run these checks automatically before each commit.

### Testing
We use `pytest` for testing.

```bash
# Run all tests
pytest
```

### Database Migrations
We use `alembic` for database schema migrations. Migrations are located in `services/common/alembic/versions`.

To apply migrations, it's best to run them inside the `signals_api` container to ensure the correct environment.
```bash
# Apply migrations
docker-compose exec signals_api alembic upgrade head
```

## Services

### Sentiment Processor
Advanced sentiment analysis using FinBERT model with batch processing capabilities.

**Features:**
- FinBERT model for financial text analysis
- Batch processing for high throughput
- Celery-based asynchronous processing
- Fallback keyword-based analysis

### Data Ingestor
Automated data collection from financial news sources.

**Features:**
- RSS feed processing
- Scheduled data collection
- Duplicate detection
- Error handling and recovery

### Signals API
RESTful API for accessing sentiment data and analytics.

**Endpoints:**
- `/api/v1/sentiment/latest` - Latest sentiment scores
- `/api/v1/sentiment/history` - Historical data
- `/api/v1/analytics/summary` - Aggregated analytics

## Deployment

### Production Deployment
1. Set environment variables in `docker-compose.prod.yml`
2. Deploy using:
   ```bash
   docker-compose -f docker-compose.prod.yml up -d
   ```

### Environment Variables
- `DATABASE_URL`: PostgreSQL connection string
- `REDIS_URL`: Redis connection string
- `API_SECRET_KEY`: API authentication key
- `LOG_LEVEL`: Logging level (INFO, DEBUG, ERROR)

## Monitoring and Logging

All services use structured JSON logging for easy parsing and monitoring. Logs are available through Docker Compose:

```bash
docker-compose logs -f service_name
```

## Contributing

1. Follow the established Docker Python import patterns
2. Add new services using the multi-stage Dockerfile template
3. Update VS Code settings for new service paths
4. Ensure all services follow the common package structure

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Support

For issues related to Docker Python imports or development environment setup, please check the troubleshooting section in the documentation.
